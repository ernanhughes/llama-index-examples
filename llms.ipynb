{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - LLMs and Prompts\n",
    "This notebook walks through testing an LLM using the primary prompt templates used in llama-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In this section, we load a test document, create an LLM, and copy prompts from llama-index to test with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load a quick document to test with. Right now, we will just load it as plain text, but we can do other operations later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/getting_started/starter_example.md\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex uses some simple templates under the hood for answering queries -- mainly a `text_qa_template` for obtaining initial answers, and a `refine_template` for refining an existing answer when all the text does not fit into one LLM call.\n",
    "\n",
    "Let's copy the default templates, and test out our LLM with a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "text_qa_template = PromptTemplate(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "refine_template = PromptTemplate(\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets test a few questions!\n",
    "\n",
    "## Text QA Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install LlamaIndex, you can follow the steps outlined in the context information under the \"Download\" section. You can clone the LlamaIndex repository by running the following command in your terminal:\n",
      "\n",
      "```bash\n",
      "$ git clone https://github.com/jerryjliu/llama_index.git\n",
      "```\n",
      "\n",
      "After cloning the repository, you can navigate to the `llama_index` directory and verify its contents. This will allow you to access the examples and get started with using LlamaIndex.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I install llama-index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index, you can follow these steps:\n",
      "\n",
      "1. Download the LlamaIndex repository and navigate to the `examples/paul_graham_essay` folder.\n",
      "2. Create a new `.py` file and import `VectorStoreIndex` and `SimpleDirectoryReader` from `llama_index`.\n",
      "3. Load the documents from the `data` folder using `SimpleDirectoryReader` and build the index using `VectorStoreIndex.from_documents(documents)`.\n",
      "4. Use the index to create a query engine with `index.as_query_engine()`.\n",
      "5. Query the index by calling `query_engine.query(\"Your query here\")`.\n",
      "\n",
      "By following these steps, you can create and query an index using LlamaIndex.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response_gen = llm.stream_complete(prompt)\n",
    "for response in response_gen:\n",
    "    print(response.delta, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "index.storage_context.persist()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "existing_answer = \"\"\"To create an index using LlamaIndex, you need to follow these steps:\n",
    "\n",
    "1. Download the LlamaIndex repository by cloning it from GitHub.\n",
    "2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n",
    "3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
    "4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.\n",
    "5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
    "6. To persist the index to disk, use `index.storage_context.persist()`.\n",
    "7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n",
    "\n",
    "Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies.\"\"\"\n",
    "prompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Example\n",
    "The LLM also has a `chat` method that takes in a list of messages, to simulate a chat session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: To create an index, you can follow these general steps:\n",
      "\n",
      "1. Define the purpose of your index: Determine what you want your index to represent and what data points you want to include.\n",
      "\n",
      "2. Gather the data: Collect the necessary data points or information that you want to include in your index.\n",
      "\n",
      "3. Calculate the index values: Depending on the type of index you are creating, you may need to calculate weighted averages, percentages, or other formulas to determine the values for each data point.\n",
      "\n",
      "4. Normalize the data: If your data points are on different scales, you may need to normalize them to ensure they are comparable.\n",
      "\n",
      "5. Combine the data points: Once you have calculated the values for each data point, combine them to create the overall index value.\n",
      "\n",
      "6. Test and validate the index: Make sure that your index accurately reflects the data and meets the intended purpose.\n",
      "\n",
      "7. Publish and maintain the index: Once your index is created, you can publish it for others to use and regularly update it to ensure it remains relevant and accurate.\n",
      "\n",
      "These steps can vary depending on the specific type of index you are creating, so be sure to tailor them to your specific needs.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful QA chatbot that can answer questions about llama-index.\"),\n",
    "    ChatMessage(role=\"user\", content=\"How do I create an index?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(chat_history)\n",
    "print(response.message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered the low-level LLM API, and tested out some basic prompts with out documentation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
